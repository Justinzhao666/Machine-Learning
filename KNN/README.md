> ## KNN k近邻学习 by lyt
>
> > > **1.核心思想**
> > >
> > > ​	给定测试样本，基于某种距离度量找出training set中与其最靠近的k个样本，然后基于这k个‘邻居’的信息进行预测，是典型的基于实例学习（instance-based learning）和懒惰学习（lazy learning）。
> > >
> > > ![KNN示意图](https://github.com/yetingli/Machine-Learning/blob/master/KNN/knn.png)
> > >
> > > **2.三要素**
> > >
> > > - k值得选择：k一般取1,3,5,7,...，k一般不超过20，k值取得不同，分类结果也就不同
> > > - 距离计算公式：欧氏距离、曼哈顿距离、...
> > > - 分类决策规则：投票法（少数服从多数，主要用于分类）、平均法（用于回归）
> > >
> > > **3.KNN优点**
> > >
> > > - 通过对k的选择，可具备丢噪声数据的健壮性（对噪声数据不敏感）
> > > - 无数据输入假定
> > >
> > > **4.KNN缺点**
> > >
> > > - 空间复杂度高（需要大量空间存储已知实例）
> > > - 计算复杂度高（需要比较所有已知实例与要分类的实例）
> > > - 当其样本分布不平衡时，比如其中一类样本过大（实例数量过多）占主导的时候，新的未知实例容易被归类为这个主导样本，因为这类样本实例的数量过大， 但这个新的未知实例实际并木接近目标样本
> > >
> > > **5.试用范围**
> > >
> > > ​	数值型和标称型
> > >
> > > **6.KNN实现**
> > >
> > > - 参见[KNN.py](https://github.com/yetingli/Machine-Learning/blob/master/KNN/KNN.py)，自己封装实现的类，步骤：加载数据、计算距离（欧氏距离）、分类预测（依据少数服从多数的投票法）、计算准确率（1-错误率）
> > > - 可以使用sklearn，调用neighbors.KNeighborsClassifier()来实现，具体见[sklearn_example.py](https://github.com/yetingli/Machine-Learning/blob/master/KNN/sklearn_example.py)
> > >
> > > **7.算法优化**
> > >
> > > - 基于距离远近进行加权平均或加权投票，如 1/d 赋投票权重，d表示距离 （在准确率上提高）
> > > - 构造kd树，减少计算距离的计算次数 （在算法复杂度上的优化）
> > >
> > > **8.参考文献**
> > >
> > > - 周志华.机器学习 第10章 降维与度量学习 10.1 k近邻学习 P225
> > > - 李航.统计学习方法 第3章 k近邻法 P37
> > > - 王斌.机器学习实战 第2章 k-紧邻算法 P15
> > > - 范淼.Python机器学习及实战 使用K紧邻分类对Iris数据进行分类P54
> > >
> > >
> > > 
> > >
> > > 
> > >
